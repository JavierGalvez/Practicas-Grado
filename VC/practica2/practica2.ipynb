{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practica2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-k0hE-XO4s"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Javier Gálvez Obispo\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"gdrive/My Drive/imagenes/\" # path usando Colab\n",
        "# path = \"./imagenes\"              # path usando Spyder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIWsHCTjPN8W"
      },
      "source": [
        "###########################################\n",
        "####### CARGAR LIBRERÍAS NECESARIAS #######\n",
        "###########################################\n",
        "\n",
        "# En caso de necesitar instalar keras en google colab,\n",
        "# ejecutar la siguiente línea:\n",
        "# !pip install -q keras\n",
        "# Importar librerías necesarias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.utils as np_utils\n",
        "\n",
        "# Importar modelos y capas que se van a usar\n",
        "from keras import Sequential, Model\n",
        "from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization, Dropout, Add, Input\n",
        "\n",
        "# Preprocesado\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Callbacks\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Importar el optimizador a usar\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# Importar el conjunto de datos\n",
        "from keras.datasets import cifar100\n",
        "\n",
        "# ResNet\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "# Eliminar Warnings\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "#######################################\n",
        "####### FUNCIONES IMPLEMENTADAS #######\n",
        "#######################################\n",
        "\n",
        "####### FUNCION DE LECTURA PARA CIFAR100 #######\n",
        "\n",
        "# A esta función solo se la llama una vez. Devuelve 4 \n",
        "# vectores conteniendo, por este orden, las imágenes\n",
        "# de entrenamiento, las clases de las imágenes de\n",
        "# entrenamiento, las imágenes del conjunto de test y\n",
        "# las clases del conjunto de test.\n",
        "def cargarImagenesCifar():\n",
        "    # Cargamos Cifar100. Cada imagen tiene tamaño\n",
        "    # (32 , 32, 3). Nos vamos a quedar con las\n",
        "    # imágenes de 25 de las clases.\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data (label_mode ='fine')\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    train_idx = np.isin(y_train, np.arange(25))\n",
        "    train_idx = np.reshape (train_idx, -1)\n",
        "    x_train = x_train[train_idx]\n",
        "    y_train = y_train[train_idx]\n",
        "    test_idx = np.isin(y_test, np.arange(25))\n",
        "    test_idx = np.reshape(test_idx, -1)\n",
        "    x_test = x_test[test_idx]\n",
        "    y_test = y_test[test_idx]\n",
        "    \n",
        "    # Transformamos los vectores de clases en matrices.\n",
        "    # Cada componente se convierte en un vector de ceros\n",
        "    # con un uno en la componente correspondiente a la\n",
        "    # clase a la que pertenece la imagen. Este paso es\n",
        "    # necesario para la clasificación multiclase en keras.\n",
        "    y_train = np_utils.to_categorical(y_train, 25)\n",
        "    y_test = np_utils.to_categorical(y_test, 25)\n",
        "    \n",
        "    return x_train , y_train , x_test , y_test\n",
        "\n",
        "####### FUNCIONES DE LECTURA PARA CALTECH-UCSD #######\n",
        "\n",
        "# Descargar las imágenes de http://www.vision.caltech.edu/visipedia/CUB-200.html\n",
        "# Descomprimir el fichero.\n",
        "# Descargar también el fichero list.tar.gz, descomprimirlo y guardar los ficheros\n",
        "# test.txt y train.txt dentro de la carpeta de imágenes anterior. Estos \n",
        "# dos ficheros contienen la partición en train y test del conjunto de datos.\n",
        "# EN CASO DE USAR COLABORATORY\n",
        "# Sube tanto las imágenes como los ficheros text.txt y train.txt a tu drive.\n",
        "\n",
        "# Dado un fichero train.txt o test.txt y el path donde se encuentran los\n",
        "# ficheros y las imágenes, esta función lee las imágenes\n",
        "# especificadas en ese fichero y devuelve las imágenes en un vector y \n",
        "# sus clases en otro.\n",
        "def leerImagenes(vec_imagenes, path):\n",
        "    clases = np.array([img.split('/')[0] for img in vec_imagenes])\n",
        "    imagenes = np.array([img_to_array(load_img(path + \"/\" + img,\n",
        "                                               target_size = (224, 224))) \n",
        "                        for img in vec_imagenes])\n",
        "    return imagenes, clases\n",
        "\n",
        "\n",
        "# Usando la función anterior, y dado el path donde se encuentran las\n",
        "# imágenes y los archivos \"train.txt\" y \"test.txt\", devuelve las \n",
        "# imágenes y las clases de train y test para usarlas con keras\n",
        "# directamente.\n",
        "def cargarDatosCaltech(path):\n",
        "    # Cargamos los ficheros\n",
        "    train_images = np.loadtxt(path + \"/train.txt\", dtype = str)\n",
        "    test_images = np.loadtxt(path + \"/test.txt\", dtype = str)\n",
        "\n",
        "    # Leemos las imágenes con la función anterior\n",
        "    train, train_clases = leerImagenes(train_images, path)\n",
        "    test, test_clases = leerImagenes(test_images, path)\n",
        "\n",
        "    # Pasamos los vectores de las clases a matrices \n",
        "    # Para ello, primero pasamos las clases a números enteros\n",
        "    clases_posibles = np.unique(np.copy(train_clases))\n",
        "    for i in range(len(clases_posibles)):\n",
        "        train_clases[train_clases == clases_posibles[i]] = i\n",
        "        test_clases[test_clases == clases_posibles[i]] = i\n",
        "\n",
        "    # Después, usamos la función to_categorical()\n",
        "    train_clases = np_utils.to_categorical(train_clases, 200)\n",
        "    test_clases = np_utils.to_categorical(test_clases, 200)\n",
        "\n",
        "    # Barajar los datos\n",
        "    train_perm = np.random.permutation(len(train))\n",
        "    train = train[train_perm]\n",
        "    train_clases = train_clases[train_perm]\n",
        "\n",
        "    test_perm = np.random.permutation(len(test))\n",
        "    test = test[test_perm]\n",
        "    test_clases = test_clases[test_perm]\n",
        "\n",
        "    return train, train_clases, test, test_clases\n",
        "\n",
        "# Función auxiliar para leer los datos desde un archivo en vez de leer cada imagen una por una\n",
        "# Ha sido necesario cambiar el método de lectura ya que en Colab tardaba demasiado en leer las imágenes\n",
        "def cargarDatosCaltech_aux(path):\n",
        "    # Leer imágenes desde binario\n",
        "    x_train = np.load(path + 'x_train.npy')\n",
        "    y_train = np.load(path + 'y_train.npy')\n",
        "    x_test = np.load(path + 'x_test.npy')\n",
        "    y_test = np.load(path + 'y_test.npy')\n",
        "\n",
        "    # Barajado de las imágenes\n",
        "    train_perm = np.random.permutation(len(x_train))\n",
        "    test_perm = np.random.permutation(len(x_test))\n",
        "    x_train = x_train[train_perm]\n",
        "    y_train = y_train[train_perm]\n",
        "    x_test = x_test[test_perm]\n",
        "    y_test = y_test[test_perm]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "###### FUNCINOES GENERALES UTLIZADAS EN TODOS LOS EJERCICIOS #######\n",
        "\n",
        "# Función para entrenar un modelo dado junto con los generadores de los conjuntos\n",
        "def entrenar(model, train, validation, epocas, early_stopping):\n",
        "  \n",
        "    callbacks = []\n",
        "\n",
        "    # Early Stopping es una de las mejoras que se pueden aplicar en el apartado 2\n",
        "    if early_stopping:\n",
        "        perdida = EarlyStopping(monitor=\"val_loss\",\n",
        "                                patience=patience, \n",
        "                                restore_best_weights=True)\n",
        "        precision = EarlyStopping(monitor=\"val_accuracy\",\n",
        "                                  patience=patience,\n",
        "                                  restore_best_weights=True)\n",
        "        callbacks.append(perdida)\n",
        "        callbacks.append(precision)\n",
        "\n",
        "    # Entrenamos\n",
        "    hist = model.fit_generator(train, epochs=epocas, verbose=1,\n",
        "                               validation_data=validation,\n",
        "                               callbacks=callbacks)\n",
        "    \n",
        "    return hist\n",
        "\n",
        "# Función para evaluar un modelo dado en el conjunto de test y mostrar los resultados\n",
        "def evaluar(model, x_test, y_test, datagen=None):\n",
        "    print(\"----------- RESULTADOS -----------\")\n",
        "    if datagen == None:\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    else:\n",
        "        score = model.evaluate_generator(datagen.flow(x_test, y_test, batch_size=1, shuffle=False),\n",
        "                                        verbose=0, steps=len(x_test))\n",
        "  \n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    print(\"----------------------------------\")\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "    return (score[0], score[1])\n",
        "\n",
        "# Esta función pinta dos gráficas, una con la evolución\n",
        "# de la función de pérdida en el conjunto de train y\n",
        "# en el de validación, y otra con la evolución de la\n",
        "# accuracy en el conjunto de train y el de validación.\n",
        "# Es necesario pasarle como parámetro el historial del\n",
        "# entrenamiento del modelo (lo que devuelven las\n",
        "# funciones fit() y fit_generator()).\n",
        "def mostrarEvolucion(hist):\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "    \n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.legend(['Training accuracy','Validation accuracy'])\n",
        "    plt.show()\n",
        "\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "# Función para comprar resultados de 2 modelos\n",
        "def compararHist(hist1, hist2, label1, label2, resultado):\n",
        "    print(\"Linea fina   = train\\nLinea gruesa = validation\\n\")\n",
        "    res1 = hist1.history[resultado]\n",
        "    val_res1 = hist1.history['val_' + resultado]\n",
        "    res2 = hist2.history[resultado]\n",
        "    val_res2 = hist2.history['val_' + resultado]\n",
        "    plt.plot(res1, color='b', lw=1.0)\n",
        "    plt.plot(val_res1, color='b', lw=3.0, label=label1)\n",
        "    plt.plot(res2, color='r', lw=1.0)\n",
        "    plt.plot(val_res2, color='r', lw=3.0, label=label2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "####### FUNCIONES DE BASENET #######\n",
        "\n",
        "### BaseNet ###\n",
        "\n",
        "# Función que devuelve el modelo de BaseNet básico\n",
        "# El parámetro batch_normalization se utiliza en el apartado 2 para poner, o no, capas de BatchNormalization\n",
        "def basenet(batch_normalization=False):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolución 5x5x6\n",
        "    model.add(Conv2D(filters=6, kernel_size=5, input_shape=input_shape))\n",
        "    if batch_normalization: model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Convolución 5x5x16\n",
        "    model.add(Conv2D(filters=16, kernel_size=5))\n",
        "    if batch_normalization: model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50))\n",
        "    if batch_normalization:\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    model.add(Dense(25, activation=\"softmax\"))\n",
        "    \n",
        "    return model\n",
        "\n",
        "### BaseNet con las mejoras del ejercicio 2 ###\n",
        "\n",
        "# Función que devuelve el modelo de BaseNet mejorado\n",
        "# El parámetro dropout se utiliza para poner, o no, capas de Dropout\n",
        "def basenet_mejorado(dropout=True):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolución 5x5x6\n",
        "    model.add(Conv2D(filters=6, kernel_size=5, input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Convolución 3x3x24\n",
        "    model.add(Conv2D(filters=24, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Convolución 5x5x16\n",
        "    model.add(Conv2D(filters=16, kernel_size=5))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Convolución 3x3x64\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # Reducimos el número de canales para seguir teniendo un vector de longitud 400\n",
        "    model.add(Conv2D(filters=16, kernel_size=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    if dropout: model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    if dropout: model.add(Dropout(0.2))\n",
        "    model.add(Dense(25, activation=\"softmax\"))\n",
        "    \n",
        "    return model\n",
        "\n",
        "### BaseNet con las mejoras del bonus ###\n",
        "\n",
        "# Función para añadir una capa de BatchNormalization y otra de ReLU a un modelo\n",
        "def bn_relu(input):\n",
        "    bn = BatchNormalization()(input)\n",
        "    relu = Activation(keras.activations.relu)(bn)\n",
        "    return relu\n",
        "\n",
        "# Función para añadir un bloque convolucional plano\n",
        "def bloque_plano(input, filters, kernel_size=3):\n",
        "    # Primera convolución\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\")(input)\n",
        "    x = bn_relu(x)\n",
        "\n",
        "    # Segunda convolución\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = bn_relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Función para añadir un bloque convolucional residual\n",
        "def bloque_residual(input, filters, kernel_size=3):\n",
        "    # Primera convolución\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\")(input)\n",
        "    x = bn_relu(x)\n",
        "\n",
        "    # Segunda convolución\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    \n",
        "    # Sumamos la entrada del bloque con la salida\n",
        "    output = Add()([input, x])\n",
        "    output = bn_relu(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Función que devuelve BaseNet con todas las mejoras del bonus\n",
        "def basenet_bonus(residual=False):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    filtros = 32\n",
        "\n",
        "    # Primera convolución 5x5\n",
        "    x = Conv2D(filters=filtros, kernel_size=5, padding=\"same\")(inputs)\n",
        "    x = bn_relu(x)\n",
        "\n",
        "    # Bloque de convoluciones\n",
        "    x =  bloque_residual(x, filters=filtros) if residual else bloque_plano(x, filters=filtros)\n",
        "\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Segunda convolución 5x5\n",
        "    x = Conv2D(filters=filtros*2, kernel_size=5, padding=\"same\")(x)\n",
        "    x = bn_relu(x)\n",
        "    \n",
        "    # Bloque de convoluciones\n",
        "    x =  bloque_residual(x, filters=filtros*2) if residual else bloque_plano(x, filters=filtros*2)\n",
        "\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = bn_relu(x)\n",
        "\n",
        "    x = Dropout(0.5)(x) \n",
        "    output = Dense(25, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, output)\n",
        "    \n",
        "    return model\n",
        "\n",
        "### Test de BaseNet ###\n",
        "\n",
        "# Función general para probar todos las variantes de BaseNet\n",
        "def probar_basenet(preprocesado, variante, sgd=False, epocas=25, summary=False, \n",
        "                   batch_normalization=False, dropout=False, early_stopping=False, \n",
        "                   residual=True):\n",
        "\n",
        "    # Cuando normalizamos standardize modifica el array original, es necesario \n",
        "    # generar una copia si queremos llamar varias veces a la función\n",
        "    test = np.copy(x_test)\n",
        "\n",
        "    # Preprocesado de los datos + Data Augmentation\n",
        "    datagen = ImageDataGenerator(**preprocesado)\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Creamos los generadores\n",
        "    train = datagen.flow(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        subset=\"training\")\n",
        "    validation = datagen.flow(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            subset=\"validation\")\n",
        "    # Si hemos normalizado train hacemos las mismas operaciones en test\n",
        "    if \"featurewise_std_normalization\" in preprocesado:\n",
        "        datagen.standardize(test)\n",
        "\n",
        "    # Creamos el modelo\n",
        "    if variante == \"BaseNet\":\n",
        "        modelo = basenet(batch_normalization)\n",
        "    elif variante == \"Mejorado\":\n",
        "        modelo = basenet_mejorado(dropout)\n",
        "    elif variante == \"Bonus\":\n",
        "        modelo = basenet_bonus(residual)\n",
        "\n",
        "    # Imprimir por pantalla el modelo\n",
        "    if summary:\n",
        "        modelo.summary()\n",
        "        input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    if sgd:\n",
        "        opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    else:\n",
        "        opt = Adam()\n",
        "\n",
        "    modelo.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                   optimizer=opt, metrics=metrica)\n",
        "\n",
        "    # Entrenamos y evaluamos el modelo\n",
        "    hist = entrenar(modelo, train, validation, epocas, early_stopping)\n",
        "    print(\"\\nEVALUAMOS EL MODELO\\n\")\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "    mostrarEvolucion(hist)\n",
        "    score = evaluar(modelo, test, y_test)\n",
        "\n",
        "    return hist, score\n",
        "\n",
        "####### FUNCIONES DE RESNET #######\n",
        "\n",
        "# Función que devuelve el modelo más sencillo posible, una capa de salida\n",
        "def resnet_simple():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(2048,)))\n",
        "    model.add(Dense(200, activation = 'softmax'))\n",
        "    return model\n",
        "\n",
        "# Función que devuelve un modelo con 2 capas fully-connected más capa de salida\n",
        "def modelo_A():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(2048,)))\n",
        "\n",
        "    # Primera capa FC\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Segunda capa FC\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Salida\n",
        "    model.add(Dense(200, activation = 'softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Función que le añade una capa convolucional al mismo modelo devuelto por modela_A()\n",
        "def modelo_B():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Capa convolucional\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, input_shape=(7, 7, 2048), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Primera capa FC\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, input_shape=(2048,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Segunda capa FC\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(keras.activations.relu))\n",
        "\n",
        "    # Salida\n",
        "    model.add(Dense(200, activation = 'softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Función que extrae las característica de un conjunto de datos\n",
        "def obtener_caracteristicas(model, datagen, x):\n",
        "    caracteristicas = model.predict_generator(datagen.flow(x, batch_size=1, shuffle=False),\n",
        "                                              verbose=1,\n",
        "                                              steps=len(x))\n",
        "    return caracteristicas\n",
        "\n",
        "### Extracción de características con ResNet50 ###\n",
        "\n",
        "# Función para utilizar ResNet50 como extractor de características\n",
        "def resnet_extractor(apartado):\n",
        "    \n",
        "    # Creamos el modelo a utilizar y configuramos ResNet50\n",
        "    if apartado == 'A':\n",
        "        pooling_type = 'avg'\n",
        "        modelo = modelo_A()\n",
        "    elif apartado == 'B':\n",
        "        pooling_type = None\n",
        "        modelo = modelo_B()\n",
        "    elif apartado == 'Base':\n",
        "        pooling_type = 'avg'\n",
        "        modelo = resnet_simple()\n",
        "\n",
        "    # Cargamos ResNet50 entrenado con imagenet\n",
        "    resnet = ResNet50(weights='imagenet', include_top=False, pooling=pooling_type)\n",
        "    \n",
        "    # Preprocesado de ResNet\n",
        "    datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
        "\n",
        "    # Extraemos características de las imágenes con el modelo anterior\n",
        "    print(\"Extrayendo caractarísticas de train\")\n",
        "    caracteristicas_train = obtener_caracteristicas(resnet, datagen, x_train)\n",
        "    print(\"Extrayendo caractarísticas de test\")\n",
        "    caracteristicas_test = obtener_caracteristicas(resnet, datagen, x_test)\n",
        "\n",
        "    modelo.summary()\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    modelo.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                   optimizer=Adam(),\n",
        "                   metrics=metrica)\n",
        "\n",
        "    # Entrenamos y evaluamos el modelo\n",
        "    hist = modelo.fit(caracteristicas_train, y_train, batch_size = batch_size,\n",
        "                      epochs = epocas, verbose = 1, validation_split = 0.1)\n",
        "    print(\"\\nEVALUAMOS EL MODELO\\n\")\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "    mostrarEvolucion(hist)\n",
        "    score = evaluar(modelo, caracteristicas_test, y_test)\n",
        "\n",
        "    return hist, score\n",
        "\n",
        "### Fine-tuning de ResNet a Caltech-UCSD ###\n",
        "\n",
        "# Función para realizar un ajuste fino de ResNet50 a un conjunto de datos\n",
        "def resnet_fine_tuning(epocas=10):\n",
        "    # Cargamos ResNet50 entrenado con imagenet\n",
        "    resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
        "\n",
        "    # Creamos los generadores\n",
        "    train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input, validation_split=0.1)\n",
        "    test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
        "\n",
        "    train = train_datagen.flow(x_train, y_train,batch_size=batch_size, subset=\"training\")\n",
        "    validation = train_datagen.flow(x_train, y_train, batch_size=batch_size, subset=\"validation\")\n",
        "\n",
        "    # Añadimos la capa de salida nueva\n",
        "    x = resnet.output\n",
        "    last = Dense(200, activation='softmax')(x)\n",
        "    modelo = Model(inputs=resnet.input, outputs=last)\n",
        "\n",
        "    # Compilamos\n",
        "    modelo.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                   optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
        "                   metrics=metrica)\n",
        "\n",
        "    # Entrenamos y evaluamos el modelo\n",
        "    hist = entrenar(modelo, train, validation, epocas=epocas, early_stopping=False)\n",
        "    print(\"\\nEVALUAMOS EL MODELO\\n\")\n",
        "    input(\"## Pulse tecla para continuar ##\\n\")\n",
        "    mostrarEvolucion(hist)\n",
        "    score = evaluar(modelo, x_test, y_test, test_datagen)\n",
        "\n",
        "    return hist, score\n",
        "\n",
        "#########################################\n",
        "####### PRUEBAS DE LOS EJERCICIOS #######\n",
        "#########################################\n",
        "\n",
        "# Variables generales\n",
        "epocas = 25                             # Número de épocas realizadas al probar un modelo\n",
        "patience = 10                           # Número de épocas sin mejoras antes de parar\n",
        "batch_size = 64                         # Tamaño del batch a utilizar\n",
        "input_shape = (32, 32 ,3)               # Dimensiones de los datos de entrada\n",
        "metrica = ['accuracy']                  # Métrica que vamos a utilizar\n",
        "preprocesado = dict()                   # Diccionario con el valor de todos los parámetros de preprocesado\n",
        "preprocesado[\"validation_split\"] = 0.1  # Porcentaje del conjunto de train que se usa para validación\n",
        "\n",
        "print(\"\\n\\n------------ Ejercicio 1 ------------\\n\\n\")\n",
        "\n",
        "# Leemos los datos de Cifar100\n",
        "print(\"Leyendo los datos de Cifar100\")\n",
        "x_train , y_train , x_test , y_test = cargarImagenesCifar()\n",
        "print(\"Datos leidos\\n\")\n",
        "\n",
        "print(\"Comparamos los resultados de BaseNet utilizando distintos optimizadores\\n\")\n",
        "\n",
        "# Probar BaseNet utilizando el SGD dado\n",
        "print(\"Probamos BaseNet con el SGD dado\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "hist_sgd, score_sgd = probar_basenet(preprocesado, \"BaseNet\", sgd=True, epocas=epocas, summary=True)\n",
        "\n",
        "# Probar BaseNet utilizando Adam\n",
        "print(\"Probamos BaseNet con Adam\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "hist_adam, score_adam = probar_basenet(preprocesado, \"BaseNet\", sgd=False, epocas=epocas)\n",
        "\n",
        "print(\"Mostramos gráficas comparando ambos resultados\")\n",
        "\n",
        "print(\"Comparamos las pérdidas en train y validation para ambos\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "compararHist(hist_sgd, hist_adam, \"SGD\", \"Adam\", \"loss\")\n",
        "\n",
        "print(\"Comparamos la precisión en train y validation para ambos\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "compararHist(hist_sgd, hist_adam, \"SGD\", \"Adam\", \"accuracy\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------ Ejercicio 2 ------------\\n\\n\")\n",
        "\n",
        "print(\"Añadimos mejoras al modelo básico de BaseNet\")\n",
        "### Sólo se ejecuta la versión definitiva con todas las mejoras ###\n",
        "\n",
        "## Normalización\n",
        "preprocesado[\"featurewise_center\"] = True\n",
        "preprocesado[\"featurewise_std_normalization\"] = True\n",
        "\n",
        "# print(\"Vemos los resultados añadiendo normalización\")\n",
        "# input(\"## Pulse tecla para continuar ##\\n\")\n",
        "# probar_basenet(preprocesado, \"BaseNet\", epocas=epocas)\n",
        "\n",
        "## Data Augmentation\n",
        "preprocesado[\"horizontal_flip\"] = True\n",
        "preprocesado[\"zoom_range\"] = 0.2\n",
        "preprocesado[\"width_shift_range\"] = 0.1\n",
        "preprocesado[\"height_shift_range\"] = 0.1\n",
        "preprocesado[\"rotation_range\"] = 20\n",
        "\n",
        "# print(\"Vemos los resultados añadiendo data augmentation\")\n",
        "# input(\"## Pulse tecla para continuar ##\\n\")\n",
        "# probar_basenet(preprocesado, \"BaseNet\", epocas=epocas)\n",
        "\n",
        "## BatchNormalization\n",
        "# print(\"Vemos los resultados añadiendo batch normalization\")\n",
        "# input(\"## Pulse tecla para continuar ##\\n\")\n",
        "# probar_basenet(preprocesado, \"BaseNet\", epocas=epocas, batch_normalization=True)\n",
        "\n",
        "## Añadimos Aumento de profundidad\n",
        "# print(\"Vemos los resultados aumentando la profundidad\")\n",
        "# input(\"## Pulse tecla para continuar ##\\n\")\n",
        "# probar_basenet(preprocesado, \"Mejorado\", epocas=epocas)\n",
        "\n",
        "## Añadimos Dropout\n",
        "# print(\"Vemos los resultados introduciendo dropout\")\n",
        "# input(\"## Pulse tecla para continuar ##\\n\")\n",
        "# probar_basenet(preprocesado, \"Mejorado\", epocas=epocas, dropout=True)\n",
        "\n",
        "## Early Stopping \n",
        "## VERSIÓN DEFINITIVA ##\n",
        "# print(\"Vemos los resultados introduciendo early stopping\")\n",
        "print(\"Versión definitiva de BaseNet con todas las mejoras\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "probar_basenet(preprocesado, \"Mejorado\", epocas=80, dropout=True, early_stopping=True, summary=True)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------ Ejercicio 3 ------------\\n\\n\")\n",
        "\n",
        "print(\"Leyendo los datos de Caltech-UCSD\")\n",
        "# Leer las imágenes una a una de las carpetas\n",
        "x_train, y_train, x_test, y_test = cargarDatosCaltech(path)\n",
        "\n",
        "# Leer directamente los numpy array ya guardados\n",
        "# x_train, y_train, x_test, y_test = cargarDatosCaltech_aux(path)\n",
        "print(\"Datos leidos\\n\")\n",
        "\n",
        "epocas = 25\n",
        "\n",
        "print(\"ResNet como extractor de características\\n\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "print(\"Sustituimos la capa de salida de ResNet\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "resnet_extractor('Base')\n",
        "\n",
        "print(\"Añadimos capas fully-connected antes de la capa de salida\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "resnet_extractor('A')\n",
        "\n",
        "print(\"Añadimos una capa de convolución antes de las capas fully-connected\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "resnet_extractor('B')\n",
        "\n",
        "\n",
        "print(\"Ajuste fino de ResNet para el conjunto de Caltech-UCSD\\n\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "resnet_fine_tuning()\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------ Bonus ------------\\n\\n\")\n",
        "\n",
        "# Leemos los datos de Cifar100\n",
        "print(\"Leyendo los datos de Cifar100\")\n",
        "x_train , y_train , x_test , y_test = cargarImagenesCifar()\n",
        "print(\"Datos leidos\\n\")\n",
        "\n",
        "print(\"BaseNet Bonus\")\n",
        "input(\"## Pulse tecla para continuar ##\\n\")\n",
        "\n",
        "epocas = 80\n",
        "# Versión plana\n",
        "# probar_basenet(preprocesado, \"Bonus\", epocas=epocas, early_stopping=True, residual=False)\n",
        "\n",
        "# Versión residual\n",
        "probar_basenet(preprocesado, \"Bonus\", epocas=epocas, early_stopping=True, residual=True, summary=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}